{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import csv\n",
    "import numpy as np \n",
    "\n",
    "# Load Yelp data from raw_data directory\n",
    "raw_data_path = './raw_data/'\n",
    "target_path = './dataset/'\n",
    "filter_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yelp_academic_dataset_business.json using polars\n",
    "business_df = pl.read_ndjson(os.path.join(raw_data_path, 'yelp_academic_dataset_business.json'))\n",
    "display(business_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business_id to category mapping\n",
    "business_categories = business_df.select(['business_id', 'categories'])\n",
    "\n",
    "# Save business_id to category mapping to TSV file\n",
    "with open(os.path.join(target_path, 'businessid2category.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for row in business_categories.iter_rows():\n",
    "        business_id, categories = row\n",
    "        if categories:  # Only write if categories exist\n",
    "            writer.writerow([business_id, categories])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>user_id</th><th>business_id</th><th>stars</th><th>useful</th><th>funny</th><th>cool</th><th>text</th><th>date</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;KU_O5udG6zpxOg-VcAEodg&quot;</td><td>&quot;mh_-eMZ6K5RLWhZyISBhwA&quot;</td><td>&quot;XQfwVwDr-v0ZS3_CbbE5Xw&quot;</td><td>3.0</td><td>0</td><td>0</td><td>0</td><td>&quot;If you decide to eat here, jus…</td><td>&quot;2018-07-07 22:09:11&quot;</td></tr><tr><td>&quot;BiTunyQ73aT9WBnpR9DZGw&quot;</td><td>&quot;OyoGAe7OKpv6SyGZT5g77Q&quot;</td><td>&quot;7ATYjTIgM3jUlt4UM3IypQ&quot;</td><td>5.0</td><td>1</td><td>0</td><td>1</td><td>&quot;I&#x27;ve taken a lot of spin class…</td><td>&quot;2012-01-03 15:28:18&quot;</td></tr><tr><td>&quot;saUsX_uimxRlCVr67Z4Jig&quot;</td><td>&quot;8g_iMtfSiwikVnbP2etR0A&quot;</td><td>&quot;YjUWPpI6HXG530lwP-fb2A&quot;</td><td>3.0</td><td>0</td><td>0</td><td>0</td><td>&quot;Family diner. Had the buffet. …</td><td>&quot;2014-02-05 20:30:30&quot;</td></tr><tr><td>&quot;AqPFMleE6RsU23_auESxiA&quot;</td><td>&quot;_7bHUi9Uuf5__HHc_Q8guQ&quot;</td><td>&quot;kxX2SOes4o-D3ZQBkiMRfA&quot;</td><td>5.0</td><td>1</td><td>0</td><td>1</td><td>&quot;Wow!&nbsp;&nbsp;Yummy, different,&nbsp;&nbsp;delic…</td><td>&quot;2015-01-04 00:01:03&quot;</td></tr><tr><td>&quot;Sx8TMOWLNuJBWer-0pcmoA&quot;</td><td>&quot;bcjbaE6dDog4jkNY91ncLQ&quot;</td><td>&quot;e4Vwtrqf-wpJfwesgvdgxQ&quot;</td><td>4.0</td><td>1</td><td>0</td><td>1</td><td>&quot;Cute interior and owner (?) ga…</td><td>&quot;2017-01-14 20:54:15&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌──────────────┬──────────────┬─────────────┬───────┬───┬───────┬──────┬─────────────┬─────────────┐\n",
       "│ review_id    ┆ user_id      ┆ business_id ┆ stars ┆ … ┆ funny ┆ cool ┆ text        ┆ date        │\n",
       "│ ---          ┆ ---          ┆ ---         ┆ ---   ┆   ┆ ---   ┆ ---  ┆ ---         ┆ ---         │\n",
       "│ str          ┆ str          ┆ str         ┆ f64   ┆   ┆ i64   ┆ i64  ┆ str         ┆ str         │\n",
       "╞══════════════╪══════════════╪═════════════╪═══════╪═══╪═══════╪══════╪═════════════╪═════════════╡\n",
       "│ KU_O5udG6zpx ┆ mh_-eMZ6K5RL ┆ XQfwVwDr-v0 ┆ 3.0   ┆ … ┆ 0     ┆ 0    ┆ If you      ┆ 2018-07-07  │\n",
       "│ Og-VcAEodg   ┆ WhZyISBhwA   ┆ ZS3_CbbE5Xw ┆       ┆   ┆       ┆      ┆ decide to   ┆ 22:09:11    │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ eat here,   ┆             │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ jus…        ┆             │\n",
       "│ BiTunyQ73aT9 ┆ OyoGAe7OKpv6 ┆ 7ATYjTIgM3j ┆ 5.0   ┆ … ┆ 0     ┆ 1    ┆ I've taken  ┆ 2012-01-03  │\n",
       "│ WBnpR9DZGw   ┆ SyGZT5g77Q   ┆ Ult4UM3IypQ ┆       ┆   ┆       ┆      ┆ a lot of    ┆ 15:28:18    │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ spin class… ┆             │\n",
       "│ saUsX_uimxRl ┆ 8g_iMtfSiwik ┆ YjUWPpI6HXG ┆ 3.0   ┆ … ┆ 0     ┆ 0    ┆ Family      ┆ 2014-02-05  │\n",
       "│ CVr67Z4Jig   ┆ VnbP2etR0A   ┆ 530lwP-fb2A ┆       ┆   ┆       ┆      ┆ diner. Had  ┆ 20:30:30    │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ the buffet. ┆             │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ …           ┆             │\n",
       "│ AqPFMleE6RsU ┆ _7bHUi9Uuf5_ ┆ kxX2SOes4o- ┆ 5.0   ┆ … ┆ 0     ┆ 1    ┆ Wow!        ┆ 2015-01-04  │\n",
       "│ 23_auESxiA   ┆ _HHc_Q8guQ   ┆ D3ZQBkiMRfA ┆       ┆   ┆       ┆      ┆ Yummy,      ┆ 00:01:03    │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ different,  ┆             │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ delic…      ┆             │\n",
       "│ Sx8TMOWLNuJB ┆ bcjbaE6dDog4 ┆ e4Vwtrqf-wp ┆ 4.0   ┆ … ┆ 0     ┆ 1    ┆ Cute        ┆ 2017-01-14  │\n",
       "│ Wer-0pcmoA   ┆ jkNY91ncLQ   ┆ JfwesgvdgxQ ┆       ┆   ┆       ┆      ┆ interior    ┆ 20:54:15    │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ and owner   ┆             │\n",
       "│              ┆              ┆             ┆       ┆   ┆       ┆      ┆ (?) ga…     ┆             │\n",
       "└──────────────┴──────────────┴─────────────┴───────┴───┴───────┴──────┴─────────────┴─────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load yelp_academic_dataset_review.json using polars\n",
    "review_df = pl.read_ndjson(os.path.join(raw_data_path, 'yelp_academic_dataset_review.json'))\n",
    "display(review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reviewerID to incremental id mapping\n",
    "user_ids = review_df['user_id'].unique()\n",
    "user_id_map = {reviewer_id: idx for idx, reviewer_id in enumerate(user_ids)}\n",
    "\n",
    "# Create asin to incremental id mapping\n",
    "business_ids = review_df['business_id'].unique()\n",
    "business_id_map = {asin: idx for idx, asin in enumerate(business_ids)}\n",
    "\n",
    "# Save mappings to files\n",
    "with open(os.path.join(target_path, 'user2idx.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for reviewer_id, idx in user_id_map.items():\n",
    "        writer.writerow([reviewer_id, idx])\n",
    "\n",
    "with open(os.path.join(target_path, 'business2idx.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for asin, idx in business_id_map.items():\n",
    "        writer.writerow([asin, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter items by popularity and users by activity in one pass\n",
    "filtered_reviews = (\n",
    "    review_df\n",
    "    .with_row_index()\n",
    "    .join(\n",
    "        review_df.group_by('business_id').len().filter(pl.col('len') >= filter_size),\n",
    "        on='business_id'\n",
    "    )\n",
    "    .join(\n",
    "        review_df.group_by('user_id').len().filter(pl.col('len') >= filter_size),\n",
    "        on='user_id'\n",
    "    )\n",
    "    .drop(['index', 'len'])\n",
    ")\n",
    "\n",
    "# Get unique user IDs and create splits\n",
    "user_ids = filtered_reviews['user_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "split_1 = int(num_users * 0.8)\n",
    "split_2 = int(num_users * 0.9)\n",
    "\n",
    "# Create train/valid/test splits using polars expressions\n",
    "train_users = set(user_ids[:split_1])\n",
    "valid_users = set(user_ids[split_1:split_2])\n",
    "test_users = set(user_ids[split_2:])\n",
    "\n",
    "train_data = filtered_reviews.filter(pl.col('user_id').is_in(train_users))\n",
    "valid_data = filtered_reviews.filter(pl.col('user_id').is_in(valid_users))\n",
    "test_data = filtered_reviews.filter(pl.col('user_id').is_in(test_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>1760170</td><td>[11818, 80995, … 53104]</td><td>[11818, 80995, … 53104]</td></tr><tr><td>1081595</td><td>[105282, 106190, … 37822]</td><td>[105282, 106190, … 37822]</td></tr><tr><td>1586128</td><td>[44340, 92836, … 4963]</td><td>[44340, 92836, … 4963]</td></tr><tr><td>184996</td><td>[83563, 40379, … 89621]</td><td>[83563, 40379, … 89621]</td></tr><tr><td>1234531</td><td>[23707, 17940, … 111285]</td><td>[23707, 17940, … 111285]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬───────────────────────────┬───────────────────────────┐\n",
       "│ user_id ┆ given_user_history        ┆ predicting_items          │\n",
       "│ ---     ┆ ---                       ┆ ---                       │\n",
       "│ i64     ┆ list[i64]                 ┆ list[i64]                 │\n",
       "╞═════════╪═══════════════════════════╪═══════════════════════════╡\n",
       "│ 1760170 ┆ [11818, 80995, … 53104]   ┆ [11818, 80995, … 53104]   │\n",
       "│ 1081595 ┆ [105282, 106190, … 37822] ┆ [105282, 106190, … 37822] │\n",
       "│ 1586128 ┆ [44340, 92836, … 4963]    ┆ [44340, 92836, … 4963]    │\n",
       "│ 184996  ┆ [83563, 40379, … 89621]   ┆ [83563, 40379, … 89621]   │\n",
       "│ 1234531 ┆ [23707, 17940, … 111285]  ┆ [23707, 17940, … 111285]  │\n",
       "└─────────┴───────────────────────────┴───────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create validation dataframe with user history and items to predict\n",
    "validation_df = (\n",
    "    valid_data\n",
    "    .group_by('user_id')\n",
    "    .agg([\n",
    "        pl.col('business_id').map_elements(lambda x: [business_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('given_user_history'),\n",
    "        pl.col('business_id').map_elements(lambda x: [business_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('predicting_items')\n",
    "    ])\n",
    "    .rename({'user_id': 'user_id'})\n",
    "    .with_columns([\n",
    "        pl.col('user_id').replace_strict(user_id_map)\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(validation_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>1760170</td><td>[11818, 80995, … 53104]</td><td>[11818, 80995, … 53104]</td></tr><tr><td>1081595</td><td>[105282, 106190, … 37822]</td><td>[105282, 106190, … 37822]</td></tr><tr><td>1586128</td><td>[44340, 92836, … 4963]</td><td>[44340, 92836, … 4963]</td></tr><tr><td>184996</td><td>[83563, 40379, … 89621]</td><td>[83563, 40379, … 89621]</td></tr><tr><td>1234531</td><td>[23707, 17940, … 111285]</td><td>[23707, 17940, … 111285]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬───────────────────────────┬───────────────────────────┐\n",
       "│ user_id ┆ given_user_history        ┆ predicting_items          │\n",
       "│ ---     ┆ ---                       ┆ ---                       │\n",
       "│ i64     ┆ list[i64]                 ┆ list[i64]                 │\n",
       "╞═════════╪═══════════════════════════╪═══════════════════════════╡\n",
       "│ 1760170 ┆ [11818, 80995, … 53104]   ┆ [11818, 80995, … 53104]   │\n",
       "│ 1081595 ┆ [105282, 106190, … 37822] ┆ [105282, 106190, … 37822] │\n",
       "│ 1586128 ┆ [44340, 92836, … 4963]    ┆ [44340, 92836, … 4963]    │\n",
       "│ 184996  ┆ [83563, 40379, … 89621]   ┆ [83563, 40379, … 89621]   │\n",
       "│ 1234531 ┆ [23707, 17940, … 111285]  ┆ [23707, 17940, … 111285]  │\n",
       "└─────────┴───────────────────────────┴───────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create validation dataframe with user history and items to predict\n",
    "test_df = (\n",
    "    test_data\n",
    "    .group_by('user_id')\n",
    "    .agg([\n",
    "        pl.col('business_id').map_elements(lambda x: [business_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('given_user_history'),\n",
    "        pl.col('business_id').map_elements(lambda x: [business_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('predicting_items')\n",
    "    ])\n",
    "    .rename({'user_id': 'user_id'})\n",
    "    .with_columns([\n",
    "        pl.col('user_id').replace_strict(user_id_map)\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(validation_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>i64</td></tr></thead><tbody><tr><td>1986308</td><td>[1335]</td><td>2846</td></tr><tr><td>1986308</td><td>[1335, 2846]</td><td>59274</td></tr><tr><td>1986308</td><td>[1335, 2846, 59274]</td><td>61237</td></tr><tr><td>1986308</td><td>[1335, 2846, … 61237]</td><td>85896</td></tr><tr><td>1986308</td><td>[1335, 2846, … 85896]</td><td>28402</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬───────────────────────┬──────────────────┐\n",
       "│ user_id ┆ given_user_history    ┆ predicting_items │\n",
       "│ ---     ┆ ---                   ┆ ---              │\n",
       "│ i64     ┆ list[i64]             ┆ i64              │\n",
       "╞═════════╪═══════════════════════╪══════════════════╡\n",
       "│ 1986308 ┆ [1335]                ┆ 2846             │\n",
       "│ 1986308 ┆ [1335, 2846]          ┆ 59274            │\n",
       "│ 1986308 ┆ [1335, 2846, 59274]   ┆ 61237            │\n",
       "│ 1986308 ┆ [1335, 2846, … 61237] ┆ 85896            │\n",
       "│ 1986308 ┆ [1335, 2846, … 85896] ┆ 28402            │\n",
       "└─────────┴───────────────────────┴──────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create training sequences with user history and next item to predict\n",
    "train_sequences = []\n",
    "for user_id, group in train_data.sort('date').group_by('user_id'):\n",
    "    history = [business_id_map[item] for item in group['business_id']]\n",
    "    user_id = user_id_map[user_id[0]]\n",
    "    \n",
    "    # Create sequences of increasing length\n",
    "    for i in range(1, len(history)):\n",
    "        train_sequences.append({\n",
    "            'user_id': user_id,\n",
    "            'given_user_history': history[:i],\n",
    "            'predicting_items': history[i]\n",
    "        })\n",
    "\n",
    "train_df = pl.DataFrame(train_sequences)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seater",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
