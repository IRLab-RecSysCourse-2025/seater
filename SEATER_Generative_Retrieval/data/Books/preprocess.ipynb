{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import csv\n",
    "\n",
    "# Load Yelp data from raw_data directory\n",
    "raw_data_path = './raw_data/'\n",
    "target_path = './dataset/'\n",
    "filter_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>category</th><th>tech1</th><th>description</th><th>fit</th><th>title</th><th>also_buy</th><th>tech2</th><th>brand</th><th>feature</th><th>rank</th><th>also_view</th><th>main_cat</th><th>similar_item</th><th>date</th><th>price</th><th>asin</th><th>imageURL</th><th>imageURLHighRes</th></tr><tr><td>list[str]</td><td>str</td><td>list[str]</td><td>str</td><td>str</td><td>list[str]</td><td>str</td><td>str</td><td>list[null]</td><td>str</td><td>list[str]</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[null]</td><td>list[null]</td></tr></thead><tbody><tr><td>[]</td><td>&quot;&quot;</td><td>[&quot;It is a biology book with God&amp;apos;s perspective.&quot;]</td><td>&quot;&quot;</td><td>&quot;Biology Gods Living Creation T…</td><td>[&quot;0669009075&quot;, &quot;B000K2P5SA&quot;, … &quot;0743252012&quot;]</td><td>&quot;&quot;</td><td>&quot;Keith Graham&quot;</td><td>[]</td><td>&quot;1,349,781 in Books (&quot;</td><td>[&quot;0019777701&quot;, &quot;B000AUCX7I&quot;, … &quot;B0095ZCRCK&quot;]</td><td>&quot;Books&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;$39.94&quot;</td><td>&quot;0000092878&quot;</td><td>[]</td><td>[]</td></tr><tr><td>[&quot;Books&quot;, &quot;New, Used &amp; Rental Textbooks&quot;, &quot;Medicine &amp; Health Sciences&quot;]</td><td>&quot;&quot;</td><td>[]</td><td>&quot;&quot;</td><td>&quot;Mksap 16 Audio Companion: Medi…</td><td>[]</td><td>&quot;&quot;</td><td>&quot;Acp&quot;</td><td>[]</td><td>&quot;1,702,625 in Books (&quot;</td><td>[&quot;B01MUCYEV7&quot;, &quot;B01KUGTY6O&quot;]</td><td>&quot;Books&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;000047715X&quot;</td><td>[]</td><td>[]</td></tr><tr><td>[&quot;Books&quot;, &quot;Arts &amp; Photography&quot;, &quot;Music&quot;]</td><td>&quot;&quot;</td><td>[&quot;Discography of American Punk, Hardcore, and Power Pop&quot;]</td><td>&quot;&quot;</td><td>&quot;Flex! Discography of North Ame…</td><td>[]</td><td>&quot;&quot;</td><td>&quot;Burkhard Jarisch&quot;</td><td>[]</td><td>&quot;6,291,012 in Books (&quot;</td><td>[]</td><td>&quot;Books&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;$199.99&quot;</td><td>&quot;0000004545&quot;</td><td>[]</td><td>[]</td></tr><tr><td>[&quot;Books&quot;, &quot;Arts &amp; Photography&quot;, &quot;Music&quot;]</td><td>&quot;&quot;</td><td>[&quot;This is a collection of classic gospel hymns that many churches still enjoy singing today.&quot;]</td><td>&quot;&quot;</td><td>&quot;Heavenly Highway Hymns: Shaped…</td><td>[]</td><td>&quot;&quot;</td><td>&quot;Stamps/Baxter&quot;</td><td>[]</td><td>&quot;2,384,057 in Books (&quot;</td><td>[&quot;0006180116&quot;, &quot;0996092730&quot;, … &quot;0871482215&quot;]</td><td>&quot;Books&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;0000013765&quot;</td><td>[]</td><td>[]</td></tr><tr><td>[]</td><td>&quot;&quot;</td><td>[]</td><td>&quot;&quot;</td><td>&quot;Georgina Goodman Nelson Womens…</td><td>[]</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>[]</td><td>&quot;11,735,726 in Books (&quot;</td><td>[]</td><td>&quot;Books&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;$164.10&quot;</td><td>&quot;0000000116&quot;</td><td>[]</td><td>[]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 18)\n",
       "┌──────────────┬───────┬──────────────┬─────┬───┬─────────┬────────────┬────────────┬──────────────┐\n",
       "│ category     ┆ tech1 ┆ description  ┆ fit ┆ … ┆ price   ┆ asin       ┆ imageURL   ┆ imageURLHigh │\n",
       "│ ---          ┆ ---   ┆ ---          ┆ --- ┆   ┆ ---     ┆ ---        ┆ ---        ┆ Res          │\n",
       "│ list[str]    ┆ str   ┆ list[str]    ┆ str ┆   ┆ str     ┆ str        ┆ list[null] ┆ ---          │\n",
       "│              ┆       ┆              ┆     ┆   ┆         ┆            ┆            ┆ list[null]   │\n",
       "╞══════════════╪═══════╪══════════════╪═════╪═══╪═════════╪════════════╪════════════╪══════════════╡\n",
       "│ []           ┆       ┆ [\"It is a    ┆     ┆ … ┆ $39.94  ┆ 0000092878 ┆ []         ┆ []           │\n",
       "│              ┆       ┆ biology book ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│              ┆       ┆ with Go…     ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ [\"Books\",    ┆       ┆ []           ┆     ┆ … ┆         ┆ 000047715X ┆ []         ┆ []           │\n",
       "│ \"New, Used & ┆       ┆              ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ Rental …     ┆       ┆              ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ [\"Books\",    ┆       ┆ [\"Discograph ┆     ┆ … ┆ $199.99 ┆ 0000004545 ┆ []         ┆ []           │\n",
       "│ \"Arts & Phot ┆       ┆ y of         ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ ography\"…    ┆       ┆ American     ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│              ┆       ┆ Punk…        ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ [\"Books\",    ┆       ┆ [\"This is a  ┆     ┆ … ┆         ┆ 0000013765 ┆ []         ┆ []           │\n",
       "│ \"Arts & Phot ┆       ┆ collection   ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ ography\"…    ┆       ┆ of clas…     ┆     ┆   ┆         ┆            ┆            ┆              │\n",
       "│ []           ┆       ┆ []           ┆     ┆ … ┆ $164.10 ┆ 0000000116 ┆ []         ┆ []           │\n",
       "└──────────────┴───────┴──────────────┴─────┴───┴─────────┴────────────┴────────────┴──────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load meta_Books.json with pandas instead of polars due to JSON parsing error\n",
    "meta_books = pl.read_ndjson(os.path.join(raw_data_path, 'meta_Books.json'))\n",
    "display(meta_books.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category2asin.tsv and brand2asin.tsv files\n",
    "category2asin = {}\n",
    "brand2asin = {}\n",
    "\n",
    "# Process meta_books to extract categories and brands\n",
    "asin2category = {}\n",
    "asin2brand = {}\n",
    "\n",
    "for row in meta_books.iter_rows(named=True):\n",
    "    asin = row['asin']\n",
    "    categories = row['category']\n",
    "    brand = row['brand'] if 'brand' in row else None\n",
    "    \n",
    "    # Handle categories\n",
    "    if categories and isinstance(categories, list):\n",
    "        asin2category[asin] = categories\n",
    "    \n",
    "    # Handle brand\n",
    "    if brand:\n",
    "        asin2brand[asin] = brand\n",
    "\n",
    "# Write asin2category.tsv\n",
    "with open(os.path.join(target_path, 'asin2category.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for asin, categories in asin2category.items():\n",
    "        writer.writerow([asin, categories])\n",
    "\n",
    "# Write asin2brand.tsv\n",
    "with open(os.path.join(target_path, 'asin2brand.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for asin, brand in asin2brand.items():\n",
    "        writer.writerow([asin, brand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>overall</th><th>vote</th><th>verified</th><th>reviewTime</th><th>reviewerID</th><th>asin</th><th>style</th><th>reviewerName</th><th>reviewText</th><th>summary</th><th>unixReviewTime</th></tr><tr><td>f64</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td><td>struct[1]</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>5.0</td><td>null</td><td>false</td><td>&quot;03 30, 2005&quot;</td><td>&quot;A1REUF3A1YCPHM&quot;</td><td>&quot;0001713353&quot;</td><td>{&quot; Hardcover&quot;}</td><td>&quot;TW Ervin II&quot;</td><td>&quot;The King, the Mice and the Che…</td><td>&quot;A story children will love and…</td><td>1112140800</td></tr><tr><td>5.0</td><td>null</td><td>true</td><td>&quot;06 20, 2016&quot;</td><td>&quot;AVP0HXC9FG790&quot;</td><td>&quot;0001713353&quot;</td><td>null</td><td>&quot;Amazon Customer&quot;</td><td>&quot;The kids loved it!&quot;</td><td>&quot;Five Stars&quot;</td><td>1466380800</td></tr><tr><td>5.0</td><td>null</td><td>true</td><td>&quot;01 24, 2016&quot;</td><td>&quot;A324TTUBKTN73A&quot;</td><td>&quot;0001713353&quot;</td><td>{&quot; Paperback&quot;}</td><td>&quot;Tekla Borner&quot;</td><td>&quot;My students (3 &amp; 4 year olds) …</td><td>&quot;Five Stars&quot;</td><td>1453593600</td></tr><tr><td>5.0</td><td>null</td><td>false</td><td>&quot;07 9, 2015&quot;</td><td>&quot;A2RE7WG349NV5D&quot;</td><td>&quot;0001713353&quot;</td><td>{&quot; Paperback&quot;}</td><td>&quot;Deborah K Woroniecki&quot;</td><td>&quot;LOVE IT&quot;</td><td>&quot;Five Stars&quot;</td><td>1436400000</td></tr><tr><td>5.0</td><td>null</td><td>true</td><td>&quot;01 18, 2015&quot;</td><td>&quot;A32B7QIUDQCD0E&quot;</td><td>&quot;0001713353&quot;</td><td>null</td><td>&quot;E&quot;</td><td>&quot;Great!&quot;</td><td>&quot;Five Stars&quot;</td><td>1421539200</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌─────────┬──────┬──────────┬─────────────┬───┬─────────────┬────────────┬────────────┬────────────┐\n",
       "│ overall ┆ vote ┆ verified ┆ reviewTime  ┆ … ┆ reviewerNam ┆ reviewText ┆ summary    ┆ unixReview │\n",
       "│ ---     ┆ ---  ┆ ---      ┆ ---         ┆   ┆ e           ┆ ---        ┆ ---        ┆ Time       │\n",
       "│ f64     ┆ str  ┆ bool     ┆ str         ┆   ┆ ---         ┆ str        ┆ str        ┆ ---        │\n",
       "│         ┆      ┆          ┆             ┆   ┆ str         ┆            ┆            ┆ i64        │\n",
       "╞═════════╪══════╪══════════╪═════════════╪═══╪═════════════╪════════════╪════════════╪════════════╡\n",
       "│ 5.0     ┆ null ┆ false    ┆ 03 30, 2005 ┆ … ┆ TW Ervin II ┆ The King,  ┆ A story    ┆ 1112140800 │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ the Mice   ┆ children   ┆            │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ and the    ┆ will love  ┆            │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ Che…       ┆ and…       ┆            │\n",
       "│ 5.0     ┆ null ┆ true     ┆ 06 20, 2016 ┆ … ┆ Amazon      ┆ The kids   ┆ Five Stars ┆ 1466380800 │\n",
       "│         ┆      ┆          ┆             ┆   ┆ Customer    ┆ loved it!  ┆            ┆            │\n",
       "│ 5.0     ┆ null ┆ true     ┆ 01 24, 2016 ┆ … ┆ Tekla       ┆ My         ┆ Five Stars ┆ 1453593600 │\n",
       "│         ┆      ┆          ┆             ┆   ┆ Borner      ┆ students   ┆            ┆            │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ (3 & 4     ┆            ┆            │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ year olds) ┆            ┆            │\n",
       "│         ┆      ┆          ┆             ┆   ┆             ┆ …          ┆            ┆            │\n",
       "│ 5.0     ┆ null ┆ false    ┆ 07 9, 2015  ┆ … ┆ Deborah K   ┆ LOVE IT    ┆ Five Stars ┆ 1436400000 │\n",
       "│         ┆      ┆          ┆             ┆   ┆ Woroniecki  ┆            ┆            ┆            │\n",
       "│ 5.0     ┆ null ┆ true     ┆ 01 18, 2015 ┆ … ┆ E           ┆ Great!     ┆ Five Stars ┆ 1421539200 │\n",
       "└─────────┴──────┴──────────┴─────────────┴───┴─────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Books_5.json using polars\n",
    "review_df = pl.read_ndjson(os.path.join(raw_data_path, 'Books_5.json'))\n",
    "display(review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reviewerID to incremental id mapping\n",
    "reviewer_ids = review_df['reviewerID'].unique()\n",
    "reviewer_id_map = {reviewer_id: idx for idx, reviewer_id in enumerate(reviewer_ids)}\n",
    "\n",
    "# Create asin to incremental id mapping\n",
    "asin_ids = review_df['asin'].unique()\n",
    "asin_id_map = {asin: idx for idx, asin in enumerate(asin_ids)}\n",
    "\n",
    "# Save mappings to files\n",
    "with open(os.path.join(target_path, 'reviewer2idx.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for reviewer_id, idx in reviewer_id_map.items():\n",
    "        writer.writerow([reviewer_id, idx])\n",
    "\n",
    "with open(os.path.join(target_path, 'asin2idx.tsv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for asin, idx in asin_id_map.items():\n",
    "        writer.writerow([asin, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter items by popularity and users by activity in one pass\n",
    "filtered_reviews = (\n",
    "    review_df\n",
    "    .with_row_index()\n",
    "    .join(\n",
    "        review_df.group_by('asin').len().filter(pl.col('len') >= filter_size),\n",
    "        on='asin'\n",
    "    )\n",
    "    .join(\n",
    "        review_df.group_by('reviewerID').len().filter(pl.col('len') >= filter_size),\n",
    "        on='reviewerID'\n",
    "    )\n",
    "    .drop(['index', 'len'])\n",
    ")\n",
    "\n",
    "# Get unique user IDs and create splits\n",
    "user_ids = filtered_reviews['reviewerID'].unique()\n",
    "num_users = len(user_ids)\n",
    "split_1 = int(num_users * 0.8)\n",
    "split_2 = int(num_users * 0.9)\n",
    "\n",
    "# Create train/valid/test splits using polars expressions\n",
    "train_users = set(user_ids[:split_1])\n",
    "valid_users = set(user_ids[split_1:split_2])\n",
    "test_users = set(user_ids[split_2:])\n",
    "\n",
    "train_data = filtered_reviews.filter(pl.col('reviewerID').is_in(train_users))\n",
    "valid_data = filtered_reviews.filter(pl.col('reviewerID').is_in(valid_users))\n",
    "test_data = filtered_reviews.filter(pl.col('reviewerID').is_in(test_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>1127753</td><td>[99711, 186712, … 65864]</td><td>[99711, 186712, … 65864]</td></tr><tr><td>1278283</td><td>[85903, 97934, … 381682]</td><td>[85903, 97934, … 381682]</td></tr><tr><td>1410122</td><td>[517959, 266308, … 437201]</td><td>[517959, 266308, … 437201]</td></tr><tr><td>1572592</td><td>[494178, 160130, … 82798]</td><td>[494178, 160130, … 82798]</td></tr><tr><td>1029558</td><td>[196981, 102686, … 667060]</td><td>[196981, 102686, … 667060]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────────────────────┬────────────────────────────┐\n",
       "│ user_id ┆ given_user_history         ┆ predicting_items           │\n",
       "│ ---     ┆ ---                        ┆ ---                        │\n",
       "│ i64     ┆ list[i64]                  ┆ list[i64]                  │\n",
       "╞═════════╪════════════════════════════╪════════════════════════════╡\n",
       "│ 1127753 ┆ [99711, 186712, … 65864]   ┆ [99711, 186712, … 65864]   │\n",
       "│ 1278283 ┆ [85903, 97934, … 381682]   ┆ [85903, 97934, … 381682]   │\n",
       "│ 1410122 ┆ [517959, 266308, … 437201] ┆ [517959, 266308, … 437201] │\n",
       "│ 1572592 ┆ [494178, 160130, … 82798]  ┆ [494178, 160130, … 82798]  │\n",
       "│ 1029558 ┆ [196981, 102686, … 667060] ┆ [196981, 102686, … 667060] │\n",
       "└─────────┴────────────────────────────┴────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create validation dataframe with user history and items to predict\n",
    "validation_df = (\n",
    "    valid_data\n",
    "    .group_by('reviewerID')\n",
    "    .agg([\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('given_user_history'),\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('predicting_items')\n",
    "    ])\n",
    "    .rename({'reviewerID': 'user_id'})\n",
    "    .with_columns([\n",
    "        pl.col('user_id').replace_strict(reviewer_id_map)\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(validation_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>352146</td><td>[107523, 697973, … 609187]</td><td>[107523, 697973, … 609187]</td></tr><tr><td>771994</td><td>[271449, 287019, … 513613]</td><td>[271449, 287019, … 513613]</td></tr><tr><td>1717550</td><td>[479653, 189544, … 91557]</td><td>[479653, 189544, … 91557]</td></tr><tr><td>859283</td><td>[399253, 277131, … 543189]</td><td>[399253, 277131, … 543189]</td></tr><tr><td>562745</td><td>[286817, 666397, … 81544]</td><td>[286817, 666397, … 81544]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────────────────────┬────────────────────────────┐\n",
       "│ user_id ┆ given_user_history         ┆ predicting_items           │\n",
       "│ ---     ┆ ---                        ┆ ---                        │\n",
       "│ i64     ┆ list[i64]                  ┆ list[i64]                  │\n",
       "╞═════════╪════════════════════════════╪════════════════════════════╡\n",
       "│ 352146  ┆ [107523, 697973, … 609187] ┆ [107523, 697973, … 609187] │\n",
       "│ 771994  ┆ [271449, 287019, … 513613] ┆ [271449, 287019, … 513613] │\n",
       "│ 1717550 ┆ [479653, 189544, … 91557]  ┆ [479653, 189544, … 91557]  │\n",
       "│ 859283  ┆ [399253, 277131, … 543189] ┆ [399253, 277131, … 543189] │\n",
       "│ 562745  ┆ [286817, 666397, … 81544]  ┆ [286817, 666397, … 81544]  │\n",
       "└─────────┴────────────────────────────┴────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create test dataframe with user history and items to predict\n",
    "test_df = (\n",
    "    test_data\n",
    "    .group_by('reviewerID')\n",
    "    .agg([\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('given_user_history'),\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('predicting_items')\n",
    "    ])\n",
    "    .rename({'reviewerID': 'user_id'})\n",
    "    .with_columns([\n",
    "        pl.col('user_id').replace_strict(reviewer_id_map)\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>given_user_history</th><th>predicting_items</th></tr><tr><td>i64</td><td>list[i64]</td><td>i64</td></tr></thead><tbody><tr><td>1813400</td><td>[152005]</td><td>147511</td></tr><tr><td>1813400</td><td>[152005, 147511]</td><td>123243</td></tr><tr><td>1813400</td><td>[152005, 147511, 123243]</td><td>33343</td></tr><tr><td>1813400</td><td>[152005, 147511, … 33343]</td><td>484312</td></tr><tr><td>1813400</td><td>[152005, 147511, … 484312]</td><td>375991</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────────────────────┬──────────────────┐\n",
       "│ user_id ┆ given_user_history         ┆ predicting_items │\n",
       "│ ---     ┆ ---                        ┆ ---              │\n",
       "│ i64     ┆ list[i64]                  ┆ i64              │\n",
       "╞═════════╪════════════════════════════╪══════════════════╡\n",
       "│ 1813400 ┆ [152005]                   ┆ 147511           │\n",
       "│ 1813400 ┆ [152005, 147511]           ┆ 123243           │\n",
       "│ 1813400 ┆ [152005, 147511, 123243]   ┆ 33343            │\n",
       "│ 1813400 ┆ [152005, 147511, … 33343]  ┆ 484312           │\n",
       "│ 1813400 ┆ [152005, 147511, … 484312] ┆ 375991           │\n",
       "└─────────┴────────────────────────────┴──────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create training dataframe with user history and items to predict\n",
    "train_df = (\n",
    "    train_data\n",
    "    .sort('unixReviewTime')  # Sort by review time\n",
    "    .group_by('reviewerID')\n",
    "    .agg([\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('given_user_history'),\n",
    "        pl.col('asin').map_elements(lambda x: [asin_id_map[item] for item in x], return_dtype=pl.List(pl.Int64)).alias('predicting_items')\n",
    "    ])\n",
    "    .rename({'reviewerID': 'user_id'})\n",
    "    .with_columns([\n",
    "        pl.col('user_id').replace_strict(reviewer_id_map)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Create sequences for each user\n",
    "train_sequences = []\n",
    "for row in train_df.iter_rows(named=True):\n",
    "    user_id = row['user_id']\n",
    "    history = row['given_user_history']\n",
    "    \n",
    "    # Create sequences of increasing length\n",
    "    for i in range(1, len(history)):\n",
    "        train_sequences.append({\n",
    "            'user_id': user_id,\n",
    "            'given_user_history': history[:i],\n",
    "            'predicting_items': history[i]\n",
    "        })\n",
    "\n",
    "# Convert to dataframe\n",
    "train_df = pl.DataFrame(train_sequences)\n",
    "\n",
    "display(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframes to TSV with list columns as comma-separated strings\n",
    "train_df.with_columns([\n",
    "    pl.col('given_user_history').map_elements(lambda x: '[' + ','.join(map(str, x)) + ']', return_dtype=pl.String),\n",
    "    pl.col('predicting_items').map_elements(lambda x: str(x), return_dtype=pl.String)  # Single item now\n",
    "]).write_csv('dataset/training.tsv', separator='\\t')\n",
    "\n",
    "validation_df.with_columns([\n",
    "    pl.col('given_user_history').map_elements(lambda x: '[' + ','.join(map(str, x)) + ']', return_dtype=pl.String),\n",
    "    pl.col('predicting_items').map_elements(lambda x: '[' + ','.join(map(str, x)) + ']', return_dtype=pl.String)\n",
    "]).write_csv('dataset/validation.tsv', separator='\\t')\n",
    "\n",
    "test_df.with_columns([\n",
    "    pl.col('given_user_history').map_elements(lambda x: '[' + ','.join(map(str, x)) + ']', return_dtype=pl.String),\n",
    "    pl.col('predicting_items').map_elements(lambda x: '[' + ','.join(map(str, x)) + ']', return_dtype=pl.String)\n",
    "]).write_csv('dataset/test.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seater",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
